# 2. Materials and Methods

## 2.1 Study Area and Species Occurrence Data

Understanding freshwater fish distributions across continental scales requires data that capture both the geographic extent of species ranges and the environmental gradients that shape them. Here, we assembled a comprehensive dataset spanning China's entire river network system, representing one of the most extensive freshwater biodiversity assessments conducted at this spatial resolution. Our approach addresses a fundamental challenge in aquatic species distribution modeling: the need to reconcile species occurrence records from heterogeneous sources with river network topology, ensuring that both presence and background data reflect the actual aquatic habitat available to organisms rather than terrestrial environments.

### 2.1.1 Continental-Scale Study Area Delineation

We delineated our study area to encompass the entire mainland China river network system, a continental-scale domain spanning **longitude 73.95°E to 134.45°E and latitude 18.25°N to 53.34°N**. This geographic extent captures extraordinary environmental diversity: from subtropical monsoon regions in the southeast characterized by high precipitation (>2000 mm annually) and warm temperatures (mean annual temperature >20°C), to arid continental climates in the northwest with precipitation <200 mm annually and extreme temperature seasonality. Elevation gradients range from sea level along the eastern coast to >5000 m in the Tibetan Plateau, creating pronounced topographic complexity that shapes both climate and hydrology.

The continental scale of our study area was not arbitrary—it was designed to enable examination of species-environment relationships across the full spectrum of spatial gradients in temperature, precipitation, topography, and anthropogenic pressures that characterize China's diverse riverine ecosystems. This breadth is critical because freshwater fish distributions are constrained by both local habitat conditions and regional climatic regimes, requiring continental-scale analyses to capture the full environmental envelope within which species can persist.

**River network pixel definition and spatial resolution**. We defined river network pixels using a flow accumulation threshold of **≥100 cells**, corresponding to approximately **~100 km² upstream contributing area**. This threshold is consistent with global hydrography standards (e.g., HydroSHEDS, HydroATLAS) and represents a balance between computational feasibility and ecological relevance. Pixels below this threshold represent ephemeral or intermittent streams that may not provide permanent aquatic habitat, while pixels above this threshold represent perennial river reaches that support sustained fish populations. This approach yielded approximately **2.1 million river pixels** across China's drainage network, providing unprecedented spatial resolution for continental-scale freshwater biodiversity modeling.

The 1-km spatial resolution of our analysis represents a significant advance over previous continental-scale freshwater SDMs, which typically operate at 5-10 km resolution. This fine-scale resolution enables us to capture local habitat heterogeneity (e.g., riparian vegetation effects, localized temperature anomalies) while maintaining computational tractability for ensemble modeling across millions of pixels. All spatial data were projected to **Albers Conic Equal Area** (centered on 105°E, 35°N), a projection that minimizes area distortion across China's latitudinal extent, ensuring that distance and area calculations remain accurate for spatial analyses.

### 2.1.2 Multi-Source Species Occurrence Data Compilation and Rigorous Quality Control

Species occurrence records for our target freshwater fish species, **Carassius auratus** (common carp), were compiled from three complementary data sources to maximize geographic coverage and minimize sampling bias: **(i) peer-reviewed taxonomic and ecological literature** documenting field surveys conducted by ichthyologists and freshwater ecologists over the past several decades; **(ii) FishBase** (www.fishbase.org), the global species information system that aggregates occurrence records from museum collections, scientific publications, and expert knowledge; and **(iii) the Global Biodiversity Information Facility (GBIF; www.gbif.org)**, the world's largest aggregator of biodiversity occurrence data, integrating records from natural history museums, citizen science platforms, and research institutions worldwide.

The initial dataset contained **n=640 raw occurrence records** downloaded from GBIF, representing the most comprehensive compilation of *C. auratus* occurrence data available for China. However, raw occurrence data from aggregator platforms are notoriously heterogeneous in quality, containing records with coordinate errors, taxonomic misidentifications, temporal inconsistencies, and spatial biases. To address these challenges, we implemented a rigorous, multi-step quality control pipeline in R (version ≥4.0.0) using the `CoordinateCleaner` and `sf` packages, following best practices established by the biodiversity informatics community.

**Step 1: Coordinate validation and range checking**. We first validated coordinate completeness and geographic plausibility. Records with missing coordinates (NA values) or coordinates outside valid ranges (longitude: -180° to 180°; latitude: -90° to 90°) were excluded, as these represent data entry errors that cannot be georeferenced. This initial validation step removed **n=0 records** (all 640 records had valid coordinate values), indicating that the GBIF dataset for *C. auratus* in China had high initial data quality. However, valid coordinate ranges do not guarantee spatial accuracy, necessitating further quality checks.

**Step 2: Spatial filtering to study area boundaries**. We restricted records to mainland China using a high-resolution national boundary shapefile (`earthenvstreams_china/china_boundary.shp`). Spatial intersection was performed using the `terra` package (`relate` function with "intersects" relation), which efficiently handles large point datasets against polygon boundaries. This step removed **n=11 records** located outside China's territorial boundaries (likely representing records from neighboring countries or marine environments misclassified as freshwater), retaining **n=629 records** within the study area. This spatial filtering is critical because species distribution models require environmental data that match the geographic extent of occurrence records, and records outside the study area would introduce extrapolation errors.

**Step 3: Coordinate precision and quality assessment**. We implemented additional quality checks to identify records with suspicious coordinate patterns that indicate data entry errors or low spatial precision. Records with identical longitude and latitude values (e.g., both coordinates = 0, or both = the same integer) were flagged as likely data entry errors, as real-world coordinates rarely exhibit such exact equality. Similarly, records with integer coordinates (e.g., 120°E, 30°N) were flagged as potentially low-precision, as these typically indicate coordinate rounding that introduces >5 km spatial uncertainty. This step removed **n=0 records** (all coordinates passed quality checks), suggesting that the GBIF dataset had been pre-processed or that coordinate precision was generally high.

**Step 4: Duplicate record identification and removal**. Spatial autocorrelation in occurrence data can arise from multiple records at identical locations, which may represent: (i) repeated sampling at the same site across different time periods, (ii) data entry duplication, or (iii) multiple specimens collected from the same location. To ensure statistical independence of occurrence records, we deduplicated records with identical coordinates, retaining only one record per unique coordinate pair. This step removed **n=32 duplicate records**, retaining **n=597 unique coordinate locations**. The relatively high proportion of duplicates (5.3% of records) suggests that some locations were intensively sampled, which is common for easily accessible or ecologically important sites.

**Step 5: Spatial thinning to reduce sampling bias and spatial autocorrelation**. A pervasive challenge in species distribution modeling is spatial sampling bias, where certain regions (e.g., easily accessible sites, protected areas, research stations) are overrepresented in occurrence datasets, while remote or inaccessible regions are undersampled. This bias can lead to spurious species-environment relationships if models learn to predict high suitability in well-sampled regions rather than actual environmental preferences. To mitigate this bias, we implemented spatial thinning using a regular grid approach: the study area was divided into grid cells of **0.09° (approximately 10 km at mid-latitudes)**, and only one record was randomly retained per grid cell using `slice_sample(n=1)`. This approach ensures that occurrence records are spatially independent at the 10-km scale, reducing the influence of sampling intensity on model predictions while maintaining geographic coverage across the species' range.

The spatial thinning step removed **n=80 records**, yielding a final dataset of **n=517 spatially independent occurrence records** for modeling. This represents a 19.2% reduction from the post-deduplication dataset, a trade-off between sample size and spatial independence that is necessary for robust statistical modeling. The final occurrence dataset spanned a geographic range of **80.67°E to 131.29°E and 18.79°N to 53.31°N**, covering all major river basins in China including the Yangtze, Yellow, Pearl, and Heilongjiang river systems. This geographic coverage ensures that our models capture the full environmental gradient across which *C. auratus* occurs, from temperate northern rivers to subtropical southern streams.

### 2.1.3 River Network-Consistent Background Point Generation

Presence-background species distribution models (also called presence-only or presence-pseudo-absence models) require not only presence records but also background points that represent "available" habitat where the species could potentially occur but has not been recorded. The selection of background points is one of the most critical decisions in presence-background modeling, as inappropriate background selection can lead to biased parameter estimates and poor model performance. Traditional SDM approaches often sample background points from the entire study area (including terrestrial environments), which is inappropriate for aquatic species that are constrained to river networks.

**River network constraint: ensuring ecological relevance of background points**. We addressed this fundamental limitation by generating background points that are restricted to river pixels (flow accumulation ≥100 cells, corresponding to ~100 km² upstream area), ensuring that background points represent "available" aquatic habitat rather than terrestrial environments where the species cannot occur. This river network-consistent sampling protocol was implemented in the `03_background_points.R` script and represents a methodological advance over traditional terrestrial SDM approaches that ignore the spatial constraints of aquatic habitat.

**Poisson-disk sampling: balancing spatial independence and geographic representativeness**. We employed **Poisson-disk sampling** on the river mask (`sampling_strategy = "poisson_disk_on_river_mask"`), an algorithm that ensures a minimum inter-point distance of **5 km** while maintaining geographic representativeness across climatic and topographic gradients. Poisson-disk sampling is superior to random or regular grid sampling because it: (i) avoids clustering of background points in easily accessible regions, (ii) ensures statistical independence by maintaining minimum distances between points, and (iii) provides uniform spatial coverage that captures environmental gradients without over-representing any particular region. This approach is particularly important for continental-scale analyses where background points must represent the full range of environmental conditions available to the species.

**Sample size determination: balancing model stability and computational efficiency**. Background points were generated at a **5:1 ratio** relative to presence points (TARGET_RATIO_BG_TO_PRES = 5), yielding **n=1680 background points** from **n=336 presence points** (after environmental variable extraction, see Section 2.2). This ratio balances multiple competing considerations: (i) model stability requires sufficient background samples for robust parameter estimation, particularly for complex machine learning algorithms that learn decision boundaries from data; (ii) computational efficiency limits the number of background points that can be processed, especially when extracting environmental data from high-resolution rasters; and (iii) statistical power for presence-background models increases with background sample size, but diminishing returns occur beyond ratios of 5-10:1. Our 5:1 ratio is consistent with recommendations for presence-background SDMs where background points should outnumber presence points by 3-10×, providing a robust foundation for model training while maintaining computational tractability.

**Spatial constraints: avoiding overlap between presence and background points**. Background points were excluded from grid cells containing presence records to avoid spatial overlap that could confound model training. This constraint ensures that presence and background points represent distinct spatial locations, preventing the model from learning spurious patterns based on proximity rather than environmental conditions. The final combined dataset contained **n=2016 observations** (336 presences + 1680 backgrounds) with complete environmental data, representing one of the largest presence-background datasets assembled for freshwater fish distribution modeling at continental scales.

## 2.2 Environmental Variable Assembly and Comprehensive Preprocessing

The success of species distribution models depends critically on the selection and preprocessing of environmental predictor variables that capture the ecological drivers of species distributions. Traditional SDM approaches often rely on readily available climate layers (e.g., WorldClim bioclimatic variables) that were developed for terrestrial applications, ignoring the unique characteristics of aquatic environments where upstream processes accumulate and propagate downstream through flow connectivity. Here, we assembled a comprehensive suite of environmental variables from four thematic domains, explicitly accounting for river network topology and upstream-downstream connectivity that shapes aquatic organism exposure to environmental conditions.

### 2.2.1 Multi-Domain Environmental Data Sources and Variable Assembly

We assembled environmental predictor variables from four complementary thematic domains to capture hydrological, climatic, topographic, and anthropogenic drivers operating at multiple spatial scales (Supplementary Table S1). This multi-domain approach addresses a fundamental limitation of traditional SDMs that focus narrowly on climate variables, ignoring the complex interactions between hydrology, topography, land use, and climate that shape freshwater ecosystems.

**Domain 1: Hydrological network topology and connectivity (3 variables)**. To quantify catchment-scale water routing and connectivity—fundamental drivers of freshwater fish distributions that determine dispersal pathways, habitat connectivity, and exposure to upstream disturbances—we extracted three key variables from river network topology layers derived from the HydroSHEDS database: **(i) flow accumulation** (upstream contributing area, in number of cells), which quantifies position within the drainage hierarchy and cumulative upstream area; **(ii) flow path length** (cumulative upstream distance, in kilometers), which measures the total distance water travels from headwaters to each pixel; and **(iii) stream order** (Strahler ordering system), which classifies river segments by their position in the drainage network hierarchy. These variables reflect not only local habitat characteristics but also cumulative upstream exposure to disturbances (e.g., pollution, land use change, climate variability) that propagate downstream through flow connectivity—a process that terrestrial SDMs cannot capture.

Flow accumulation was obtained from **EarthEnv-Streams Version 1.0** at 1-km resolution, a global dataset that provides standardized river network topology derived from high-resolution digital elevation models. The flow accumulation variable is particularly important because it determines both habitat size (larger rivers support larger fish populations) and connectivity (high flow accumulation indicates mainstem rivers that serve as dispersal corridors). We log-transformed flow accumulation to normalize its highly skewed distribution (most pixels have low flow accumulation, while a few pixels have very high values representing major river confluences).

**Domain 2: River network-weighted hydroclimatic variables (18 variables)**. A critical innovation of our approach is the use of **EarthEnv-Streams Version 1.0**, a global dataset providing 1-km resolution hydrological and climatic metrics that are **weighted by upstream catchment area**. This network-weighting approach fundamentally differs from traditional terrestrial grid-based climate layers (e.g., WorldClim, CHELSA), which assign climate values based solely on the local grid cell, ignoring upstream accumulation effects that are central to aquatic organism exposure.

For each river pixel, we extracted **monthly mean temperature and precipitation** (12 months: January through December) that were aggregated as **upstream-area-weighted averages**. This weighting means that climate conditions at each pixel reflect not only local conditions but also the integrated climatic exposure from the entire upstream catchment, capturing how temperature and precipitation patterns propagate downstream through flow connectivity. For example, a downstream pixel experiences the cumulative effects of upstream temperature regimes, with warm upstream conditions raising downstream temperatures through advective heat transport, while cold upstream conditions provide thermal refugia that moderate downstream temperatures.

Additionally, we computed **quarterly hydroclimatic summaries** (4 quarters: Q1=Dec-Feb, Q2=Mar-May, Q3=Jun-Aug, Q4=Sep-Nov) to represent seasonal variability in temperature and precipitation regimes. These quarterly summaries capture intra-annual patterns that are critical for understanding species distributions, as many freshwater fish species have specific thermal and flow requirements during different life history stages (e.g., spawning, juvenile growth, overwintering).

Variable names follow the convention `hydro_wavg_XX`, where XX indicates the month (01-12) or quarter (13-24). This network-weighting approach represents a paradigm shift from terrestrial to aquatic SDM frameworks, explicitly accounting for the directional flow of water, energy, and materials through river networks—processes that are central to aquatic ecology but absent from traditional terrestrial modeling approaches.

**Domain 3: Topographic gradients and terrain complexity (8 variables)**. Terrain metrics were derived from the **SRTM 90-m digital elevation model (DEM)** aggregated to 1-km resolution to match our river network data. Topographic variables capture energy gradients, flow velocity potential, and habitat heterogeneity along longitudinal stream profiles—factors that directly influence fish distributions through their effects on water temperature, flow velocity, substrate composition, and habitat complexity.

We computed eight topographic variables: **(i) elevation mean** (`dem_avg`), representing the average elevation within each 1-km pixel; **(ii) elevation range** (`dem_range`), quantifying local topographic relief that influences habitat heterogeneity; **(iii) slope mean** (`slope_avg`), measuring average terrain steepness that determines flow velocity and substrate stability; **(iv) slope range** (`slope_range`), capturing local slope variability; **(v) topographic position index (TPI)**, quantifying whether a pixel is located in a valley, on a slope, or on a ridge relative to surrounding terrain; **(vi) terrain ruggedness index (TRI)**, measuring local topographic complexity that influences habitat diversity; and two additional terrain metrics that capture aspect and curvature effects on solar radiation and flow accumulation.

These topographic variables are particularly important for understanding fish distributions because they determine both physical habitat characteristics (e.g., pool-riffle sequences, substrate composition) and thermal regimes (e.g., valley bottoms are cooler due to cold air drainage, while exposed slopes receive more solar radiation). The longitudinal gradient from headwaters (high elevation, steep slopes) to lowlands (low elevation, gentle slopes) represents one of the most fundamental environmental gradients in riverine ecosystems, shaping species distributions through multiple mechanisms.

**Domain 4: Upstream-area-weighted land cover and soil properties (18 variables)**. Land use and land cover in upstream catchments profoundly influence downstream aquatic ecosystems through multiple pathways: **(i) nutrient and sediment export** from agricultural and urban areas affects water quality and habitat structure; **(ii) riparian vegetation** provides shade that moderates water temperature and supplies organic matter that supports food webs; **(iii) impervious surfaces** in urban areas increase runoff and reduce baseflow, altering hydrologic regimes; and **(iv) forest cover** regulates water yield and timing, affecting flow variability.

To capture these upstream effects, we computed **upstream-area-weighted land cover fractions** for 12 classes from the **Consensus Land Cover product** at 300-m resolution, aggregated to 1 km. Land cover classes included: evergreen broadleaf forest, deciduous broadleaf forest, mixed forest, shrubland, grassland, cropland, urban built-up, barren land, snow/ice, open water, wetland, and other. Variable names follow `lc_wavg_XX`, where XX indicates land cover class code (01-12). The upstream-area-weighting ensures that land cover at each pixel reflects the cumulative land use patterns from the entire upstream catchment, not just local conditions—a critical distinction for aquatic organisms that experience the integrated effects of upstream land use.

**Soil properties** from **SoilGrids250m** were similarly weighted by upstream catchment area, including: pH, organic carbon content, cation exchange capacity, bulk density, and texture (sand/silt/clay fractions) at 0–30 cm depth. These variables represent edaphic controls on nutrient export and sediment dynamics that shape downstream water quality and habitat conditions. Variable names follow `soil_wavg_XX`, where XX indicates soil property code (01-07). The upstream-area-weighting of soil properties captures how soil characteristics throughout the catchment influence downstream water chemistry and sediment loads, processes that are central to aquatic ecosystem functioning but typically ignored in terrestrial SDM frameworks.

### 2.2.2 Systematic Variable Prescreening and Multicollinearity Control

From an initial pool of **>100 candidate predictors** extracted from EarthEnv-Streams netCDF files and other global datasets, we implemented a systematic, multi-step reduction workflow to retain variables with acceptable multicollinearity while maintaining ecological comprehensiveness. This reduction process is critical because high-dimensional predictor sets with strong correlations can lead to: (i) model overfitting, where algorithms learn spurious patterns from correlated variables; (ii) unstable parameter estimates, where small changes in data lead to large changes in coefficients; (iii) inflated variance in predictions, reducing model reliability; and (iv) computational challenges for some algorithms (e.g., neural networks, causal discovery) that are sensitive to multicollinearity.

**Step 1: Zero-variance and near-zero-variance exclusion**. We first identified and excluded variables with zero variance (constant values across all sampling points) or near-zero variance (coefficient of variation <0.01). These variables provide no discriminatory power for modeling because they cannot distinguish between presence and background locations—if a variable has the same value everywhere, it cannot explain spatial variation in species distributions. This step removed variables that were either: (i) constant across the study area (e.g., some land cover classes that do not occur in China), (ii) nearly constant with minimal spatial variation (e.g., some soil properties with very low variability), or (iii) artifacts of data processing that introduced constant values.

**Step 2: Pairwise correlation screening with strict threshold**. We computed **Pearson correlation coefficients** for all variable pairs using the `cor()` function with `method="pearson"` and `use="complete.obs"` to handle missing values. For each pair with |r| >0.8, we removed the variable with lower ecological interpretability or higher missingness, retaining the variable that was more directly interpretable or had better data quality. This threshold (|r| <0.8) is **stricter than commonly used thresholds** (|r| <0.7 or 0.9) to ensure greater variable independence, reducing multicollinearity risks in machine learning models that can be sensitive to correlated predictors.

The pairwise correlation screening revealed several expected patterns: (i) monthly temperature variables were highly correlated (r >0.9) due to seasonal autocorrelation, leading us to retain quarterly summaries that capture seasonal patterns with less redundancy; (ii) elevation and temperature were negatively correlated (r ≈ -0.7) as expected from lapse rate relationships, but below our 0.8 threshold; (iii) some land cover classes were negatively correlated (e.g., forest vs. cropland, r ≈ -0.6) but remained below the threshold, allowing us to retain both variables that capture distinct land use patterns.

**Step 3: Iterative Variance Inflation Factor (VIF) analysis**. To further control multicollinearity beyond pairwise correlations (which only capture bivariate relationships and miss higher-order multicollinearity), we calculated **Variance Inflation Factor (VIF)** values iteratively using the `usdm::vif()` function. VIF quantifies how much the variance of a regression coefficient is inflated due to multicollinearity with other predictors, with VIF = 1 indicating no multicollinearity and VIF >10 indicating problematic multicollinearity that can destabilize parameter estimates.

We implemented an iterative removal process: variables were removed one at a time, starting with the highest VIF, and VIF values were recalculated after each removal until all remaining variables had **VIF ≤10**, a standard threshold indicating acceptable multicollinearity. The iterative process continued until convergence (no variables with VIF >10 remained). This iterative approach is necessary because VIF values change as variables are removed—a variable with initially high VIF may become acceptable after removing its highly correlated partners.

**Final variable set: balancing comprehensiveness and statistical independence**. This systematic reduction workflow yielded **47 predictors** spanning all four domains, with **mean pairwise correlation of 0.34 (SD=0.28)** and **maximum VIF of 8.7** (well below the threshold of 10). The final variable set achieved an optimal balance between: (i) **ecological comprehensiveness**, capturing key environmental axes (hydrological, climatic, topographic, anthropogenic) that shape species distributions; and (ii) **statistical independence**, minimizing confounding and multicollinearity that can compromise model interpretability and reliability.

All continuous variables were **standardized** (z-score transformation: `scale()`, mean=0, SD=1) before modeling to: (i) enable cross-variable comparison of effect sizes, as variables measured on different scales (e.g., temperature in °C, elevation in m, flow accumulation in cells) cannot be directly compared without standardization; (ii) satisfy distributional assumptions of some algorithms (e.g., neural networks perform better with standardized inputs, causal discovery algorithms require approximate Gaussianity); and (iii) improve numerical stability and convergence for iterative optimization algorithms.

**Variable grouping for visualization and interpretation**. The 47 variables were organized into four groups for visualization and ecological interpretation: **(i) Topography & Flow (G1_TopoSlopeFlow, n=11 variables)**, including elevation, slope, terrain indices, and flow accumulation metrics; **(ii) Hydroclimatic (G2_Hydroclim_wavg, n=18 variables)**, including monthly and quarterly temperature and precipitation weighted by upstream catchment area; **(iii) Land Cover (G3_Landcover_wavg, n=12 variables)**, including upstream-weighted fractions of 12 land cover classes; and **(iv) Soil Properties (G4_Soil_wavg, n=6 variables)**, including upstream-weighted soil chemistry and texture variables. This grouping reflects both data source (variables from the same dataset are grouped together) and ecological process domains (variables that capture similar ecological mechanisms are grouped together), facilitating interpretation of model results and variable importance patterns.

### 2.2.3 Environmental Data Extraction and Missing Value Treatment

Environmental variable values were extracted for each occurrence and background point (n=2016 total observations) using the `raster::extract()` function with **bilinear interpolation** for continuous variables. Bilinear interpolation was chosen over nearest-neighbor extraction because it provides smoother, more accurate estimates of environmental conditions at point locations, particularly when points fall between grid cell centers. This interpolation method calculates values as weighted averages of the four nearest grid cells, reducing discretization artifacts that can arise from nearest-neighbor extraction.

**Missing value identification and exclusion criteria**. Missing values (NoData) in raster layers can arise from multiple sources: (i) data gaps in source datasets (e.g., cloud cover in satellite imagery, incomplete survey coverage); (ii) pixels outside the spatial extent of certain variables (e.g., some land cover classes may not be mapped in certain regions); (iii) processing artifacts (e.g., edge effects, reprojection issues); and (iv) quality control flags that mark unreliable data. We implemented a conservative missing value treatment strategy: **(i) excluding pixels with >20% missing variables** (n=0 pixels in final dataset), as high missingness indicates data quality issues that cannot be reliably imputed; and **(ii) imputing remaining missing values (<5% of data)** using **median values** computed from non-missing observations within the same geographic region (10-km radius).

**Regional median imputation: minimizing bias while maintaining sample size**. The regional median imputation approach (using 10-km radius neighborhoods) minimizes bias compared to global median imputation because it accounts for spatial autocorrelation in environmental variables—missing values are imputed using values from nearby locations that are likely to have similar environmental conditions. This approach is particularly appropriate for variables with strong spatial structure (e.g., elevation, temperature, land cover), where nearby locations are more similar than distant locations. The 10-km radius was chosen to balance: (i) spatial proximity (ensuring imputed values reflect local conditions) and (ii) sample size (ensuring sufficient non-missing observations for reliable median estimation).

This conservative imputation approach minimizes bias while maintaining sample size, ensuring that we retain the maximum number of observations for model training while avoiding the introduction of spurious patterns from inappropriate imputation. The fact that <5% of data required imputation, and no pixels had >20% missingness, indicates high data quality across our environmental variable suite—a critical foundation for reliable species distribution modeling.

## 2.3 Regional-Scale Hydrological Variability and Network-Weighted Environmental Gradients

A fundamental challenge in freshwater species distribution modeling is capturing how environmental conditions vary not only locally but also across regional scales through upstream-downstream connectivity and catchment-wide processes. Traditional terrestrial SDM approaches treat each grid cell independently, ignoring the directional flow of water, energy, and materials through river networks that shapes aquatic organism exposure to environmental conditions. Here, we explicitly account for regional-scale hydrological variability by using network-weighted environmental variables that capture upstream accumulation effects—a methodological innovation that addresses a critical gap in aquatic SDM frameworks.

### 2.3.1 The Network-Weighting Paradigm: From Local to Catchment-Scale Environmental Exposure

The core innovation of our approach is the recognition that aquatic organisms experience environmental conditions not only at their immediate location but also through the integrated effects of upstream catchment processes. This network-weighting paradigm fundamentally differs from traditional terrestrial SDM frameworks, which assign environmental values based solely on local grid cell conditions. For aquatic species, this local-only approach is inadequate because: (i) **water flows downstream**, carrying with it temperature, nutrients, pollutants, and other materials from upstream areas; (ii) **catchment processes accumulate**, with land use, climate, and topography throughout the upstream catchment influencing downstream conditions; and (iii) **connectivity matters**, as organisms can disperse along river networks, experiencing environmental gradients that span entire catchments rather than isolated grid cells.

We implemented this network-weighting approach using **EarthEnv-Streams Version 1.0**, which provides 1-km resolution environmental variables that are **weighted by upstream catchment area**. For each river pixel, environmental values reflect the area-weighted average of conditions throughout the entire upstream catchment, calculated as: `weighted_value = Σ(upstream_value_i × catchment_area_i) / Σ(catchment_area_i)`, where the summation includes all upstream pixels that drain to the focal pixel. This weighting ensures that large upstream areas (e.g., major tributaries) have proportionally greater influence on downstream conditions than small upstream areas (e.g., headwater streams), reflecting the actual hydrologic processes that shape aquatic environments.

**Temperature and precipitation: capturing climatic propagation through river networks**. Monthly mean temperature and precipitation variables (12 months) were aggregated as upstream-area-weighted averages, capturing how climatic conditions propagate downstream through flow connectivity. This approach reveals patterns that are invisible to local-only analyses: for example, a downstream pixel may experience moderate local temperatures but high upstream-area-weighted temperatures if warm conditions prevail throughout the upstream catchment, leading to elevated downstream temperatures through advective heat transport. Similarly, precipitation patterns throughout the upstream catchment determine downstream flow regimes, with high upstream precipitation leading to increased downstream discharge even if local precipitation is low.

The quarterly hydroclimatic summaries (4 quarters) capture seasonal variability in these network-weighted climatic patterns, revealing how upstream-downstream connectivity shapes intra-annual environmental regimes. For example, spring snowmelt in high-elevation headwaters leads to increased downstream flow and decreased temperature during Q2 (Mar-May), even in lowland areas where local conditions are warmer—a pattern that local-only climate variables cannot capture.

**Land cover and soil properties: quantifying anthropogenic and edaphic influences**. Upstream-area-weighted land cover fractions capture how land use patterns throughout the catchment influence downstream aquatic ecosystems. For example, high upstream cropland coverage leads to increased nutrient and sediment export downstream, affecting water quality and habitat structure even in pixels with local forest cover. Similarly, upstream urban areas increase impervious surface coverage, leading to altered hydrologic regimes (increased peak flows, reduced baseflow) that propagate downstream through the river network.

Soil properties weighted by upstream catchment area quantify edaphic controls on nutrient export and sediment dynamics that shape downstream water chemistry and habitat conditions. For example, upstream areas with high soil organic carbon content contribute dissolved organic matter to downstream waters, affecting light penetration, nutrient availability, and food web structure. The network-weighting ensures that these upstream effects are properly accounted for in species distribution models, addressing a fundamental limitation of terrestrial SDM approaches that ignore catchment-scale processes.

### 2.3.2 Spatial Patterns of Hydrological Variability Across China's River Networks

The network-weighted environmental variables reveal pronounced spatial patterns of hydrological variability across China's river networks that reflect both climatic gradients and topographic complexity. **Elevation gradients** from the Tibetan Plateau (>5000 m) to coastal lowlands (sea level) create strong temperature and precipitation gradients that are captured by network-weighted variables: high-elevation headwaters experience cold temperatures and variable precipitation (snow-dominated), while lowland mainstem rivers integrate these upstream conditions with local warm, humid climates.

**Longitudinal gradients** from headwaters to river mouths represent another fundamental axis of environmental variation: headwater streams (low flow accumulation) are characterized by steep slopes, high elevation, cold temperatures, and forest-dominated catchments, while downstream mainstem rivers (high flow accumulation) are characterized by gentle slopes, low elevation, warm temperatures, and mixed land use (agricultural and urban). These longitudinal gradients shape fish distributions through multiple mechanisms: (i) **thermal regimes** vary from cold headwaters (suitable for cold-water species) to warm lowlands (suitable for warm-water species); (ii) **habitat complexity** decreases from heterogeneous headwater reaches (pool-riffle sequences, diverse substrates) to homogeneous lowland reaches (deep, slow-flowing channels with fine substrates); and (iii) **connectivity** increases from isolated headwater streams (limited dispersal) to well-connected mainstem rivers (extensive dispersal corridors).

**Latitudinal gradients** from subtropical southern China to temperate northern China create additional environmental variation: southern rivers experience warm, humid climates year-round with high precipitation (>2000 mm annually), while northern rivers experience cold winters, warm summers, and moderate precipitation (500-1000 mm annually). These latitudinal gradients interact with longitudinal gradients to create complex spatial patterns: for example, southern headwater streams are warmer than northern headwater streams, but both are cooler than their respective lowland mainstem rivers.

The network-weighting approach captures how these spatial gradients interact through upstream-downstream connectivity: a lowland pixel in southern China experiences not only local warm, humid conditions but also the integrated effects of warm, humid conditions throughout its upstream catchment, leading to consistently high network-weighted temperatures. In contrast, a lowland pixel in northern China experiences local moderate temperatures but also the integrated effects of cold headwater conditions, leading to lower network-weighted temperatures that reflect the full catchment thermal regime.

### 2.3.3 Implications for Species Distribution Modeling

The network-weighting paradigm has profound implications for species distribution modeling in freshwater ecosystems. By explicitly accounting for upstream-downstream connectivity and catchment-scale processes, our approach captures environmental gradients that are central to aquatic organism distributions but invisible to traditional terrestrial SDM frameworks. This methodological innovation addresses a fundamental gap in aquatic SDM research, where environmental variables have historically been borrowed from terrestrial frameworks without modification, ignoring the unique characteristics of riverine ecosystems.

The 47-variable predictor set, with its emphasis on network-weighted hydroclimatic, land cover, and soil variables, provides a comprehensive representation of the environmental drivers that shape freshwater fish distributions across continental scales. This variable set balances: (i) **local habitat conditions** (e.g., elevation, slope, local land cover) that determine immediate habitat suitability; (ii) **catchment-scale processes** (e.g., network-weighted temperature, precipitation, land use) that shape downstream environmental conditions; and (iii) **connectivity metrics** (e.g., flow accumulation, stream order) that determine dispersal potential and habitat accessibility.

This comprehensive environmental representation sets the stage for robust species distribution modeling that can capture both local and regional drivers of species distributions, enabling predictions that reflect the actual environmental gradients experienced by aquatic organisms as they move through river networks and experience the integrated effects of upstream catchment processes.

## 2.4 Species Distribution Model Training and Validation

With comprehensive occurrence data and network-weighted environmental variables in hand, we faced the challenge of building robust predictive models that could capture complex species-environment relationships while maintaining interpretability and generalizability. Traditional SDM approaches often rely on single algorithms that make restrictive assumptions about functional forms, limiting their ability to capture the diverse mechanisms through which environmental conditions shape species distributions. Here, we adopted a multi-algorithm ensemble approach, training four complementary modeling algorithms that span the spectrum from parametric to non-parametric, linear to highly nonlinear, providing both robust predictions and mechanistic insights into species-environment relationships.

### 2.4.1 Stratified Data Partitioning for Robust Model Evaluation

The foundation of reliable model evaluation is a rigorous data partitioning strategy that ensures independent assessment of model performance on data that were not used during training. We partitioned our combined dataset (n=2016 observations: 336 presences + 1680 backgrounds) into stratified training (80%, n=1613) and independent test (20%, n=403) sets using `caret::createDataPartition()` with `p=0.8` and `list=FALSE`. The stratification preserved prevalence ratios (presence:background ≈ 1:5) within each partition, ensuring that both training and test sets represent the full environmental gradient across which the species occurs. This stratified approach is critical because random partitioning can lead to imbalanced prevalence ratios between partitions, biasing model evaluation metrics that are sensitive to class imbalance.

**Random seed fixation for reproducibility**. The random seed was fixed (`set.seed(20251024)`) to ensure complete reproducibility of the data partitioning, enabling exact replication of our results and facilitating comparison with future studies. This reproducibility is essential for scientific rigor, as stochastic data partitioning can lead to different training-test splits that yield different performance estimates, complicating interpretation and comparison across studies.

**Test set composition and independence**. The independent test set contained **n=403 observations**, including **n=67 presence records** (16.6% prevalence) and **n=336 background records** (83.4%). This test set was held out during all model training and hyperparameter tuning procedures and used exclusively for final performance evaluation. The strict separation of training and test data prevents data leakage—a critical issue in machine learning where information from test data inadvertently influences model training, leading to overly optimistic performance estimates that do not generalize to new data. By maintaining this strict separation, we ensure that our performance metrics reflect true predictive ability rather than overfitting to the training data.

The 20% test set size represents a balance between: (i) **statistical power** for reliable performance estimation (larger test sets provide more precise estimates), and (ii) **training data availability** (larger training sets enable more complex models to learn species-environment relationships). With n=1613 training observations and 47 predictors, our sample-to-variable ratio (34:1) exceeds recommended thresholds (>10:1) for stable model training, while the n=403 test observations provide sufficient statistical power to detect meaningful differences in model performance.

### 2.4.2 Multi-Algorithm Ensemble Modeling: Capturing Methodological Diversity

We trained four complementary SDM algorithms, selected to span methodological diversity and capture different assumptions about species-environment relationships. This multi-algorithm approach addresses a fundamental limitation of single-algorithm studies, which can yield misleading conclusions if the chosen algorithm makes assumptions that are violated by the data. By training multiple algorithms with different assumptions, we can: (i) identify robust patterns that are consistent across algorithms, (ii) quantify structural uncertainty arising from algorithmic choices, and (iii) leverage ensemble predictions that combine the strengths of different approaches while mitigating individual weaknesses.

**(i) Maximum Entropy (Maxent): Regularized Logistic Regression with Flexible Feature Transformations**. We implemented Maxent via the `maxnet` R package (version 0.1.4), which fits regularized logistic regression with flexible feature transformations. Maxent is based on the principle of maximum entropy, finding the probability distribution that is most uniform (maximum entropy) while satisfying constraints imposed by the observed occurrence data and environmental variables. This approach is particularly well-suited for presence-background data because it does not require absence records and naturally handles class imbalance.

Model specification: `maxnet(presence ~ ., data=train_data, regmult=1.0, feature_classes="lqph")`, where `regmult=1.0` controls regularization strength (default value that balances model fit and complexity), and `feature_classes="lqph"` enables four types of feature transformations: **linear (l)** for monotonic relationships, **quadratic (q)** for unimodal responses, **product (p)** for variable interactions, and **hinge (h)** for threshold effects. These flexible feature transformations allow Maxent to capture complex nonlinear species-environment relationships without requiring explicit specification of functional forms.

Model outputs were converted to logistic probability scale (range: 0-1) using `predict(maxnet_model, newdata=test_data, type="logistic")`, providing interpretable habitat suitability scores that can be directly compared across models and used for conservation planning. Maxent excels at capturing complex nonlinear responses with limited overfitting risk through regularization, which penalizes complex models that may fit training data noise rather than true species-environment relationships. The regularization parameter (`regmult=1.0`) was set to the default value, which has been shown to provide good performance across diverse species and environmental contexts.

**(ii) Random Forest (RF): Ensemble Tree-Based Learning with Automatic Interaction Detection**. A Random Forest ensemble of **500 regression trees** was trained using the `randomForest` package (version 4.7-1.1) with specification: `randomForest(x=X_train, y=y_train, ntree=500, mtry=sqrt(p), nodesize=5, importance=TRUE)`, where `p=47` is the number of predictors, `mtry=sqrt(47)≈7` controls variable sampling per tree (default for classification, ensuring each tree uses a random subset of predictors), and `nodesize=5` sets minimum node size (controlling tree depth and preventing overfitting).

Random Forest operates by growing multiple decision trees via bootstrap aggregating (bagging), where each tree is trained on a bootstrap sample of the training data and uses a random subset of predictors at each split. This dual randomization (bootstrap sampling + random predictor selection) ensures that trees are diverse, reducing correlation between trees and improving ensemble performance. The final prediction is the average (for regression) or majority vote (for classification) across all trees, providing robust predictions that are less sensitive to outliers and noise than individual trees.

**Out-of-bag (OOB) error** provided internal validation, as each tree's OOB predictions (from observations not in its bootstrap sample) provide unbiased performance estimates without requiring a separate validation set. Variable importance was computed using **permutation-based importance** (`importance(type=1)`), which measures the increase in prediction error when each variable is randomly permuted, breaking its association with the response. This importance metric captures both direct effects and interactions, as permuting a variable involved in interactions will increase prediction error even if it has no direct effect.

RF handles interactions and nonlinearity without parametric assumptions, automatically detecting complex relationships that would require explicit specification in parametric models. The algorithm is robust to collinearity due to random variable subsampling, as correlated variables are unlikely to be selected together in the same tree, reducing multicollinearity effects. However, RF offers limited interpretability compared to GAM or linear models, as the ensemble of trees does not provide simple functional forms that can be easily interpreted.

**(iii) Generalized Additive Model (GAM): Interpretable Smooth Functions with Spatial Smoothing**. We fitted a binomial GAM using the `mgcv` package (version 1.8-42) with penalized thin-plate regression splines. GAMs extend generalized linear models by allowing smooth, nonlinear functions of predictors, providing a flexible yet interpretable framework for modeling species-environment relationships. Model specification: `gam(presence ~ s(var1, k=5) + s(var2, k=5) + ... + te(lon, lat, k=5), data=train_data, family=binomial(link="logit"), method="REML", weights=case_weights)`, where `k=5` sets the number of basis functions per smooth term (penalized to prevent overfitting), `method="REML"` (Restricted Maximum Likelihood) automatically penalizes overfitting through smoothness selection, and `te(lon, lat, k=5)` is a bivariate tensor product smooth capturing residual spatial autocorrelation.

The smooth terms `s(var1, k=5)` allow each environmental variable to have a flexible, data-driven response curve that can capture unimodal, threshold, or other nonlinear relationships without requiring explicit specification of functional forms. The `k=5` parameter sets the maximum complexity of each smooth (5 basis functions), but the actual complexity is determined automatically through penalized likelihood, preventing overfitting while allowing sufficient flexibility to capture true species-environment relationships.

**Spatial smoothing** via the bivariate tensor product smooth `te(lon, lat, k=5)` captures residual spatial autocorrelation that is not explained by environmental variables. This spatial term is critical because species distributions often exhibit spatial structure due to: (i) dispersal limitations that create spatial clustering, (ii) unmeasured environmental variables that vary spatially, and (iii) historical contingencies that create spatial patterns independent of current environmental conditions. By explicitly modeling this spatial structure, GAM can improve predictions and provide insights into the relative importance of environmental vs. spatial drivers of species distributions.

**Class imbalance** was addressed by assigning case weights (`presence:background = 3:1`) to upweight presence records, ensuring that the model focuses on correctly predicting presences rather than being overwhelmed by the more numerous background points. This weighting is particularly important for GAMs, which can be sensitive to class imbalance when using maximum likelihood estimation. GAM provides interpretable smooth functions and explicit spatial smoothing, enabling mechanistic interpretation of species-environment relationships through visualization of response curves and spatial patterns.

**(iv) Single-Hidden-Layer Neural Network (NN): Flexible Nonlinear Decision Boundaries**. A feedforward neural network with **10 hidden units** was trained via backpropagation using the `nnet` package (version 7.3-19) with specification: `nnet(x=X_train_scaled, y=y_train, size=10, decay=0.01, maxit=500, trace=FALSE)`, where `size=10` sets the number of hidden units (controlling model complexity), `decay=0.01` controls weight decay (L2 regularization) to prevent overfitting, and `maxit=500` sets maximum iterations for the optimization algorithm.

Neural networks can approximate arbitrary nonlinear decision boundaries through their ability to learn complex combinations of inputs via hidden layer transformations. The single hidden layer with 10 units provides sufficient flexibility to capture nonlinear species-environment relationships while maintaining computational tractability and reducing overfitting risk compared to deeper networks. Inputs were z-score standardized (`scale()`) to ensure that all variables contribute equally to the model, as neural networks are sensitive to input scaling. Outputs were logistic probabilities, providing interpretable habitat suitability scores comparable to other algorithms.

Variable importance was computed using **Garson's algorithm** (`NeuralNetTools::garson()`), which decomposes the network's connection weights to quantify the relative contribution of each input variable to the output. This importance metric provides insights into which environmental variables drive predictions, although the black-box nature of neural networks limits detailed interpretation of functional forms. NN can approximate arbitrary nonlinear decision boundaries but offers limited interpretability compared to GAM or RF, making it valuable for prediction but less useful for mechanistic understanding.

### 2.4.3 Comprehensive Model Evaluation Metrics

Independent test-set performance was assessed using multiple complementary metrics computed using the `pROC` package (version 1.18.0), providing a comprehensive evaluation of model discrimination, calibration, and classification accuracy. This multi-metric approach is essential because different metrics capture different aspects of model performance, and a model that excels in one metric may perform poorly in another.

**(i) Area under the receiver operating characteristic curve (AUC): Threshold-Independent Discrimination**. AUC measures discrimination across all probability thresholds, ranging from 0.5 (random performance, diagonal ROC curve) to 1.0 (perfect discrimination, ROC curve reaches top-left corner). AUC is threshold-independent, making it particularly useful for presence-background models where the optimal classification threshold is often unknown and may vary depending on conservation objectives.

AUC values >0.7 indicate acceptable discrimination, >0.8 indicate good discrimination, and >0.9 indicate excellent discrimination. AUC was computed using `roc(test_data$presence, test_data$predicted, quiet=TRUE)` to construct the ROC curve, and `auc(roc_object)` to calculate the area under the curve. The ROC curve plots sensitivity (true positive rate) against 1-specificity (false positive rate) across all possible probability thresholds, providing a visual representation of the trade-off between correctly identifying presences and incorrectly classifying backgrounds as presences.

**(ii) True Skill Statistic (TSS): Threshold-Dependent Classification Accuracy**. TSS = sensitivity + specificity - 1, a threshold-dependent metric that balances omission errors (failing to predict presences) and commission errors (incorrectly predicting presences). TSS ranges from -1 (worst possible performance) to +1 (perfect performance), with values >0.4 indicating acceptable performance, >0.6 indicating good performance, and >0.8 indicating excellent performance.

TSS was computed at the optimal classification threshold determined via Youden's index (see below), which maximizes TSS by balancing sensitivity and specificity. Unlike AUC, TSS is threshold-dependent, requiring specification of a classification threshold to convert continuous probability predictions into binary presence/absence predictions. This threshold-dependent nature makes TSS more directly interpretable for conservation applications where binary predictions are needed, but also more sensitive to threshold selection.

**(iii) Sensitivity and Specificity: Component Metrics of Classification Accuracy**. **Sensitivity** (true positive rate) = TP/(TP+FN), measuring the proportion of actual presences correctly predicted. High sensitivity is critical for conservation applications where failing to identify suitable habitat (false negatives) can lead to missed conservation opportunities. **Specificity** (true negative rate) = TN/(TN+FP), measuring the proportion of actual absences correctly predicted. High specificity is important when commission errors (false positives) lead to wasted conservation resources or inappropriate management actions.

Both metrics were computed at the optimal classification threshold, providing detailed insights into the types of errors made by each model. A model with high sensitivity but low specificity may be useful for initial surveys where missing suitable habitat is more costly than investigating unsuitable areas, while a model with high specificity but low sensitivity may be preferred when conservation resources are limited and false positives are costly.

**Optimal classification threshold determination via Youden's index**. Classification thresholds were determined via **Youden's index**, which maximizes TSS: `threshold = coords(roc_object, "best", ret="threshold", best.method="youden")`. Youden's index identifies the threshold that maximizes the sum of sensitivity and specificity, equivalent to finding the point on the ROC curve that is farthest from the diagonal (random performance). This approach balances sensitivity and specificity, avoiding bias toward either metric and providing a data-driven threshold selection that is appropriate for most conservation applications.

**Additional metrics: Boyce index for calibration assessment**. We computed **Boyce indices** to evaluate predicted-to-expected frequency ratios across probability bins, providing a **continuous Boyce index (CBI)** that measures prediction reliability independent of prevalence. The Boyce index compares the frequency of observed presences in each predicted probability bin to the frequency expected under a null model, with values >0.5 indicating reliable predictions where presences are concentrated in high-probability bins. CBI values close to 1.0 indicate excellent calibration, where predicted probabilities accurately reflect the true probability of species presence, while values <0 indicate poor calibration where the model performs worse than random.

## 2.5 Causal Inference Framework: From Correlation to Causation

While species distribution models excel at identifying predictive relationships between environmental variables and species occurrences, they cannot distinguish correlation from causation—a fundamental limitation that constrains mechanistic understanding and conservation applications. Two variables may be correlated because: (i) one directly causes the other (A→B), (ii) one indirectly affects the other through intermediate variables (A→C→B), or (iii) both are affected by a common cause (A←C→B, confounding). Traditional variable importance metrics cannot distinguish these scenarios, leading to misinterpretation of species-environment relationships and suboptimal conservation strategies.

Here, we advance from correlative variable importance to causal pathway identification using Bayesian network-based causal discovery and causal effect estimation. This framework enables us to: (i) identify direct vs. indirect causal pathways through which environmental variables affect species distributions, (ii) quantify causal effects while controlling for confounding, and (iii) map spatially heterogeneous treatment effects that reveal where environmental interventions yield largest impacts. This causal inference approach represents a paradigm shift in SDM research, moving from "what predicts species distributions" to "what causes species distributions"—a distinction that is critical for mechanistic understanding and evidence-based conservation.

### 2.5.1 Causal Structure Learning: Discovering Environmental Causal Networks

To discover the causal structure underlying species-environment relationships, we applied Bayesian network-based causal discovery to the 47-variable environmental dataset. Bayesian networks represent variables as nodes and causal relationships as directed edges, forming directed acyclic graphs (DAGs) that encode conditional independence relationships. This framework distinguishes: **(i) direct effects** (A→B, where A directly causes B), **(ii) indirect effects** (A→C→B, where A affects B through intermediate variable C), and **(iii) confounded associations** (A←C→B, where A and B are correlated due to common cause C but have no direct causal relationship).

**Data preparation for causal discovery**. Causal structure learning was performed on standardized training-set environmental data (n=1603 samples × 47 variables). Continuous variables were standardized (z-score transformation) to satisfy distributional assumptions of causal discovery algorithms, particularly the PC algorithm which requires approximate Gaussianity for reliable partial correlation tests. Standardization also ensures that all variables are on comparable scales, preventing variables with larger variances from dominating the analysis.

**Algorithm 1: Constraint-based PC algorithm for local independence testing**. The PC (Peter-Clark) algorithm, implemented in the `pcalg` package (version 2.7-8), infers DAGs via conditional independence tests. The algorithm operates in two phases: **(i) skeleton identification**, where edges are removed if conditional independence holds (partial correlation |r| <threshold given separating sets), starting from a fully connected graph and progressively testing independence given larger conditioning sets; and **(ii) edge orientation**, where edge directions are determined via v-structure rules (A→C←B patterns indicating causal colliders) and other orientation rules that propagate directional information through the graph.

We used partial correlation tests with significance level **α=0.01** (stricter than commonly used α=0.05 to reduce false positive edges) and maximum conditioning set size of **3** (balancing statistical power and computational feasibility). Larger conditioning sets provide more powerful independence tests but increase computational cost exponentially, while smaller conditioning sets are computationally efficient but may miss higher-order conditional independencies. The algorithm was applied to the covariance matrix: `pc(suffStat=list(C=cor(X_scaled), n=nrow(X_scaled)), indepTest=gaussCItest, alpha=0.01, labels=colnames(X_scaled))`.

The PC algorithm's strength lies in its principled approach to causal discovery based on conditional independence, which is theoretically grounded in causal Markov and faithfulness assumptions. However, the algorithm can be sensitive to sample size limitations and may miss edges when conditional independence tests lack statistical power.

**Algorithm 2: Score-based Hill-Climbing algorithm for global optimization**. The Hill-Climbing (HC) algorithm, implemented in the `bnlearn` package (version 4.8.3), searches DAG space via greedy optimization of Bayesian Information Criterion (BIC) scores, balancing model fit (log-likelihood) against complexity (edge count). Starting from an empty graph, the algorithm iteratively adds, removes, or reverses edges to maximize BIC until convergence (no improvement possible). BIC penalizes model complexity, preventing overfitting by favoring simpler graphs that explain the data well.

For continuous data, we used the **BIC-G score** (Gaussian likelihood): `hc(data=X_scaled, score="bic-g")`. If BIC-G failed due to numerical issues (e.g., singular covariance matrices), we fell back to **BGE (Bayesian Gaussian Equivalent) score**: `hc(data=X_scaled, score="bge")`, which is more robust to numerical instabilities. This approach complements PC by directly optimizing global structure rather than local independence, and is more robust to sample size limitations because it uses likelihood-based scoring rather than conditional independence tests that can lack power with limited data.

The Hill-Climbing algorithm's strength lies in its ability to directly optimize global structure, potentially discovering causal relationships that PC might miss due to statistical power limitations. However, the greedy search can get trapped in local optima, and the algorithm may require multiple random restarts to explore the DAG space thoroughly.

### 2.5.2 Bootstrap Stability Assessment: Quantifying Causal Edge Robustness

Causal discovery algorithms can be sensitive to sampling variability, with small changes in data leading to different inferred causal structures. To quantify edge robustness and mitigate single-sample instability, we applied both algorithms to **300 bootstrap replicates** of the training data. For each replicate, we sampled **80% of observations** with replacement (`m = floor(0.8 * nrow(X_scaled))`), ensuring sufficient sample size (n≈1282 observations) while introducing variability that tests the stability of causal relationships.

Bootstrap resampling was performed using `bnlearn::boot.strength()` with specification: `boot.strength(data=X_scaled, R=300, algorithm="hc", algorithm.args=list(score="bic-g"), m=floor(0.8*nrow(X_scaled)))`. The 300 replicates provide sufficient statistical power to reliably estimate edge stability, with standard errors of stability estimates decreasing as the square root of the number of replicates.

**Edge stability calculation and threshold selection**. For each directed edge (A→B), stability (strength) was computed as the proportion of bootstrap DAGs containing that edge. Edges with stability ≥0.55 (appearing in >55% of replicates, i.e., 165/300 replicates) were retained in the averaged causal network. This threshold (0.55) is **more conservative than commonly used thresholds** (0.5), ensuring only robust causal dependencies supported across data perturbations are retained, while filtering spurious edges arising from sampling variability.

The 0.55 threshold represents a balance between: (i) **retaining true causal relationships** that are robust across data perturbations (higher thresholds risk filtering true edges), and (ii) **filtering spurious edges** that arise from sampling variability or algorithmic artifacts (lower thresholds risk retaining false edges). This conservative approach prioritizes precision (fewer false positives) over recall (fewer false negatives), which is appropriate for causal discovery where false causal claims can lead to misguided conservation strategies.

**Averaged network construction via consensus**. We constructed an averaged causal network using `bnlearn::averaged.network(boot_strength, threshold=0.55)`, which retains edges with stability ≥threshold and orients them based on majority consensus across bootstrap replicates. This ensemble approach reveals consensus causal structures supported across data perturbations, providing more robust and reliable causal networks than single-sample inference. The averaged network represents the "core" causal structure that is consistently supported across different data samples, filtering out edges that are sensitive to sampling variability.

### 2.5.3 Average Treatment Effect (ATE) Estimation: Quantifying Causal Effects

While causal structure learning identifies *which* variables causally affect species distributions, it does not quantify *how much* each variable affects species presence—information that is critical for prioritizing conservation interventions. To quantify causal effects of environmental variables on species presence, we estimated **Average Treatment Effects (ATE)** using **Double Machine Learning (DML)**, implemented via the `DoubleML` package (version 0.6.2). DML enables unbiased estimation of causal effects in observational data by leveraging machine learning to flexibly control for high-dimensional confounders, addressing a fundamental challenge in causal inference where traditional methods (e.g., linear regression) may fail when confounders are numerous and relationships are nonlinear.

**Treatment definition and binarization**. We defined "treatment" as standardized increases in key environmental drivers (e.g., +1 SD in upstream-weighted temperature). For each candidate variable, treatment was binarized at the median value (high vs. low exposure), creating two groups: treatment group (above median) and control group (below median). This binarization simplifies interpretation (comparing high vs. low exposure) while maintaining sufficient contrast to detect causal effects. ATE was estimated as the difference in expected species presence probability between treatment and control groups, adjusted for all other environmental variables (confounders).

**DML specification: orthogonalized residual approach**. For each variable, we fitted two nuisance functions using Random Forest: **(i) outcome model**: `E[Y|X, T]`, predicting species presence from all covariates X and treatment T; and **(ii) treatment model**: `E[T|X]`, predicting treatment assignment from covariates X. These nuisance functions flexibly control for confounding using machine learning, capturing complex nonlinear relationships that traditional methods (e.g., linear regression) cannot.

ATE was estimated using the **orthogonalized residual approach**: `ATE = E[(Y - E[Y|X]) * (T - E[T|X])] / E[(T - E[T|X])²]`, which eliminates confounding bias by residualizing both the outcome and treatment on confounders before computing the treatment effect. This approach is robust to misspecification of nuisance functions (Neyman orthogonality), providing unbiased ATE estimates even when the outcome or treatment models are imperfectly specified.

**Statistical inference and significance testing**. Standard errors were computed using robust sandwich estimators to account for heteroscedasticity (non-constant error variance). Significance was assessed using two-sided t-tests with **α=0.05 threshold (p<0.05)**. **95% confidence intervals** were computed as: `CI = ATE ± 1.96 × SE`, where SE is the standard error. Variables with p<0.05 and |ATE| >0.05 were considered significantly causally effective, where the |ATE| >0.05 threshold ensures that only variables with practically meaningful effects (≥5 percentage point change in species presence probability) are considered significant.

**Batch ATE estimation for systematic causal identification**. We performed batch ATE estimation across the top 20 variables ranked by predictive importance, enabling systematic identification of variables with significant causal effects. This systematic approach ensures that we do not miss important causal drivers that may have been overlooked in exploratory analyses. Results were summarized in `output/14_causal/ate_all_variables.csv`, including ATE coefficient, standard error, p-value, and 95% confidence interval for each variable, providing a comprehensive catalog of causal effects that can guide conservation prioritization.

### 2.5.4 Conditional Average Treatment Effect (CATE) Estimation: Mapping Spatially Heterogeneous Intervention Potential

While ATE quantifies average causal effects across all observations, it masks spatial heterogeneity in treatment responses—information that is critical for conservation planning where interventions must be prioritized across geographic space. For example, riparian restoration may yield large habitat suitability gains in some regions (high-CATE zones) but minimal gains in others (low-CATE zones), depending on local environmental contexts. To reveal this spatial heterogeneity, we estimated **Conditional Average Treatment Effects (CATE)**, which quantify treatment effects conditional on covariate values, revealing *where* interventions yield largest impacts.

**Causal forest specification for heterogeneous treatment effects**. We estimated pixel-specific treatment effects using **causal forests**, implemented via the `grf` package (version 2.3.0). Causal forests are a non-parametric machine learning method that learns heterogeneous treatment responses without assuming parametric functional forms. We trained causal forests using `causal_forest(X=X_train, Y=y_train, W=treatment_train, num.trees=2000, min.node.size=5)`, where X contains all environmental covariates, Y is species presence, and W is treatment assignment.

The algorithm uses ensemble regression trees that partition covariate space to maximize treatment effect heterogeneity, identifying regions where treatment effects are largest (high-CATE) or smallest (low-CATE). The 2000 trees provide robust estimates of heterogeneous treatment effects, while `min.node.size=5` prevents overfitting by requiring minimum sample sizes in terminal nodes. Causal forests are particularly powerful for conservation applications because they can identify complex interactions between treatment and environmental covariates that determine where interventions are most effective.

**Spatial CATE mapping for conservation prioritization**. For each river pixel in China's network (~2.1 million pixels), we predicted CATE using `predict(causal_forest, newdata=X_river_pixels)`, producing spatially explicit maps of intervention potential. High-CATE regions indicate where environmental changes (e.g., riparian restoration reducing temperature, flow regulation dampening hydrologic extremes) yield disproportionately large habitat suitability gains, enabling prioritization of conservation investments to maximize impact per unit effort.

These CATE maps transform abstract causal effects into actionable conservation guidance, revealing not just *what* environmental variables causally affect species distributions, but *where* interventions targeting these variables will be most effective. This spatial prioritization is critical for conservation planning where resources are limited and must be allocated to maximize conservation outcomes.

**CATE distribution statistics and interpretation**. CATE distributions across all river pixels were summarized using descriptive statistics: **mean (μ)=0.0134**, **standard deviation (σ)=0.0446**, **median (50th percentile)=0.0128**, and quantiles (**10th percentile=-0.0456**, **90th percentile=0.0639**). The range of CATE values (-0.046 to +0.064) indicates substantial spatial heterogeneity in intervention potential, with high-CATE regions (90th percentile, CATE>0.064) representing "high-leverage" zones where environmental improvements yield disproportionately large habitat suitability gains.

Negative-CATE regions (10th percentile, CATE<-0.046) suggest potential habitat traps where environmental improvements may yield limited benefits under current climatic regimes, requiring alternative conservation strategies (e.g., urban heat island mitigation, stormwater management) that address different mechanisms. This spatial heterogeneity in treatment effects reveals that conservation strategies must be tailored to local environmental contexts, as one-size-fits-all interventions will be suboptimal when treatment effects vary spatially.

## 2.6 Future Climate Projections and Uncertainty Quantification

Predicting how species distributions will respond to future climate change is one of the most pressing challenges in conservation biology, requiring models that can reliably extrapolate beyond observed environmental conditions. However, future SDM projections face a fundamental tension: models trained on comprehensive environmental variable sets (e.g., our 47-variable suite including network-weighted land cover, soil properties) may achieve high predictive performance for current conditions, but many of these variables are unavailable or unreliable for future scenarios, creating extrapolation risks that can lead to unreliable projections.

Here, we address this challenge through a "scenario-consistent variable retraining" strategy that ensures spatial and temporal transferability by using only variables available across both current and future time slices. We then project habitat suitability across China's river network under multiple climate scenarios, quantifying and decomposing uncertainty sources to provide robust, actionable predictions for conservation planning under climate change.

### 2.6.1 Scenario-Consistent Variable Selection and Model Retraining

A pervasive challenge in future SDM projections is **extrapolation beyond training data's environmental bounds** when predictor variables unavailable in future scenarios (e.g., upstream-weighted land cover under dynamic urbanization, soil properties under changing land use) are included. Extrapolation occurs when models make predictions for environmental conditions outside the range of training data, where species-environment relationships are unknown and may differ from relationships within the training range. This extrapolation risk is particularly acute for variables that are difficult to project into the future (e.g., land use, which depends on socioeconomic factors that are highly uncertain).

To ensure spatial and temporal transferability, we adopted a **"scenario-available variable retraining" strategy**: all four models (Maxent, RF, GAM, NN) were retrained using only **6 predictors** available across both current (1970–2000) and future (2041–2060) time slices: **(i) mean annual temperature (bio01)**, representing overall thermal regime; **(ii) total annual precipitation (bio12)**, representing overall water availability; **(iii) temperature seasonality (bio04, SD of monthly means)**, representing intra-annual temperature variability; **(iv) precipitation seasonality (bio15, CV of monthly totals)**, representing intra-annual precipitation variability; **(v) elevation (dem_avg)**, representing topographic position; and **(vi) slope (slope_avg)**, representing terrain steepness.

This reduced feature space sacrifices predictive resolution (6 variables vs. 47 variables) but eliminates extrapolation into unobserved covariate combinations, a critical requirement for reliable projections. The 6-variable set captures fundamental climatic and topographic drivers that are: (i) available in future climate scenarios (temperature, precipitation), (ii) static over decadal timescales (elevation, slope), and (iii) ecologically meaningful for freshwater fish distributions (thermal and hydrologic regimes, topographic habitat structure).

**Future climate data source: CMIP6 downscaled projections**. We obtained downscaled CMIP6 climate data at 1-km resolution from WorldClim 2.1 for four **Shared Socioeconomic Pathways (SSPs)** representing different trajectories of greenhouse gas emissions and socioeconomic development: **SSP1-2.6** (low forcing, +1.8°C global warming by 2100, representing rapid decarbonization and sustainable development), **SSP2-4.5** (intermediate, +2.7°C, representing moderate emissions and mixed development), **SSP3-7.0** (high, +3.6°C, representing high emissions and regional rivalry), and **SSP5-8.5** (very high, +4.4°C, representing very high emissions and fossil-fueled development).

Mid-century (2041–2060) averages across **23 GCM ensemble members** (BCC-CSM2-MR model) provided probabilistic climate projections that capture both mean climate change and internal climate variability. The 23-member ensemble enables quantification of climate model uncertainty, although we note that using a single GCM (BCC-CSM2-MR) limits our ability to fully characterize climate model structural uncertainty. Static topographic variables (elevation, slope) were held constant across time slices, consistent with negligible geomorphic change over decadal timescales (geomorphic processes operate on millennial timescales).

**Model retraining and performance validation**. Retrained models (Maxent, RF, GAM, NN) using 6 scenario-available variables were evaluated on the same independent test set (n=403) to confirm maintained predictive performance despite variable reduction. All retrained models achieved AUC ≥0.87 (RF: 0.913, Maxent: 0.906, GAM: 0.878, NN: 0.872), demonstrating that scenario-consistent retraining maintains discrimination despite variable reduction. This performance retention indicates that the 6-variable set captures sufficient environmental information to predict species distributions, although we acknowledge that the reduced variable set may miss some fine-scale habitat drivers (e.g., local land cover, soil properties) that contribute to current predictions but are unavailable for future scenarios.

### 2.6.2 Spatial Projection and Change Analysis

For each SSP×model combination (4 scenarios × 4 models = **16 projections**), we predicted habitat suitability across China's river network pixels (~2.1 million pixels) using `predict(model, newdata=future_env_data, type="response")`. Predictions were computed at 1-km resolution and masked to river network pixels (flow accumulation ≥100 cells), ensuring that projections reflect only aquatic habitat where the species can potentially occur.

**Summary statistics for characterizing distribution shifts**. We computed pixel-wise summary statistics (mean, SD, quantiles: 10th, 50th, 90th percentiles) across all 16 projections to characterize distribution shifts under climate change. Mean habitat suitability changes were calculated as: **Δ = (mean_suitability_future - mean_suitability_current) / mean_suitability_current × 100%**, providing percentage change metrics that are interpretable for conservation planning. These summary statistics reveal: (i) **mean changes** indicating overall distribution shifts (expansion or contraction), (ii) **spatial variability** (SD) indicating heterogeneity in climate change impacts across the river network, and (iii) **quantiles** revealing the distribution of impacts, including extreme changes that may represent critical conservation priorities.

**Change mapping for identifying expansion and contraction zones**. We mapped pixel-wise changes (future - current) to identify expansion/contraction zones where habitat suitability increases or decreases under climate change. **Positive changes** indicate habitat suitability increases (potential range expansion), representing regions where climate change creates more favorable conditions for the species. **Negative changes** indicate habitat suitability decreases (potential range contraction), representing regions where climate change creates less favorable conditions that may lead to local extirpation.

These change maps enable spatial prioritization of conservation actions: expansion zones may represent opportunities for assisted colonization or natural range shifts, while contraction zones may require immediate conservation interventions (e.g., habitat restoration, flow management) to prevent local extinctions. The spatial resolution (1-km) enables identification of fine-scale refugia and expansion corridors that may be missed in coarser-resolution projections.

### 2.6.3 Uncertainty Quantification and Decomposition

Future SDM projections are subject to multiple sources of uncertainty that can substantially affect conservation decisions: (i) **structural uncertainty** arising from different modeling algorithms making different assumptions about species-environment relationships, (ii) **scenario uncertainty** arising from different emission pathways and socioeconomic trajectories, and (iii) **climate model uncertainty** arising from different GCMs representing climate system processes differently. Quantifying and decomposing these uncertainty sources is critical for robust conservation planning, as high-uncertainty regions require more cautious interpretation and may benefit from adaptive management strategies.

**Cross-model variance (structural uncertainty)**. For each SSP scenario, we computed the **standard deviation of predictions across the four models** (Maxent, RF, GAM, NN), reflecting algorithmic structural uncertainty. This quantifies how much predictions vary due to different modeling assumptions (e.g., linear vs. nonlinear, parametric vs. non-parametric, with vs. without spatial smoothing). High structural uncertainty indicates that predictions are sensitive to algorithmic choices, requiring multi-model ensembles to provide robust projections. Low structural uncertainty indicates that predictions are consistent across algorithms, providing greater confidence in projection reliability.

**Cross-scenario variance (scenario uncertainty)**. For each model, we computed the **standard deviation of predictions across the four SSP scenarios** (SSP126, SSP245, SSP370, SSP585), reflecting climate forcing uncertainty. This quantifies how much predictions vary due to different emission pathways, revealing the sensitivity of species distributions to greenhouse gas emissions trajectories. High scenario uncertainty indicates that conservation outcomes depend critically on emission reductions, while low scenario uncertainty indicates that species distributions are relatively insensitive to emission pathways (within the range of scenarios considered).

**Model agreement for consensus assessment**. We computed model agreement as: `agreement = 1 - (max_prediction - min_prediction)`, ranging from 0 (total disagreement, max-min=1) to 1 (perfect consensus, max-min=0). High agreement (agreement >0.85) indicates robust predictions supported across models, providing greater confidence for conservation planning. Low agreement (agreement <0.50) indicates high structural uncertainty requiring caution in interpretation, as predictions vary substantially depending on algorithmic choices.

**Variance partitioning for uncertainty source identification**. We performed variance partitioning to quantify the relative contribution of structural uncertainty vs. scenario uncertainty to total prediction variance. This enables identification of whether conservation planning should prioritize: (i) **multi-model ensembles** (if structural uncertainty dominates, as different algorithms yield different predictions), or (ii) **scenario proliferation** (if scenario uncertainty dominates, as different emission pathways yield different predictions). This partitioning guides resource allocation in conservation planning, focusing efforts where they will have the greatest impact on reducing prediction uncertainty.

**Results: structural uncertainty dominates prediction variance**. Structural uncertainty (cross-model variance) was quantified as the standard deviation of predictions across the four models (Maxent, RF, GAM, NN) for each pixel under fixed SSP scenarios. Mean structural uncertainty across all river pixels was **SD=0.0987** (median=0.0549, range: 5.95×10⁻⁶ to 0.530). Scenario uncertainty (cross-scenario variance) was quantified as the standard deviation of predictions across the four SSP scenarios (SSP126, SSP245, SSP370, SSP585) for each pixel under fixed models. Mean scenario uncertainty was **SD=0.0021** (substantially lower than structural uncertainty).

Structural uncertainty exceeded scenario uncertainty by **4.7-fold** (0.0987/0.0021), indicating that **algorithmic choices dominate prediction variance over emission scenarios**. This finding has profound implications for conservation planning: (i) multi-model ensembles are essential for robust projections, as structural uncertainty is the dominant source of prediction variance; (ii) scenario uncertainty is relatively minor, suggesting that species distributions may be relatively insensitive to emission pathways within the range of scenarios considered; and (iii) conservation planning should prioritize multi-model ensembles over scenario proliferation to reduce prediction uncertainty.

Model agreement (1 - (max - min)) had mean=0.780 (median=0.882, range: 3.95×10⁻⁹ to 0.999), with **78% of pixels showing agreement >0.85**, indicating high spatial consensus across models. This high agreement suggests that, despite structural uncertainty being the dominant source of variance, models generally agree on the direction and magnitude of climate change impacts across most of the river network. However, the 22% of pixels with lower agreement (<0.85) represent regions where algorithmic choices lead to substantially different predictions, requiring cautious interpretation and potentially benefiting from additional data collection or model refinement.

## 2.7 Visualization and Figure Standards

All figures were generated following Nature journal specifications to ensure publication-ready quality:

**Resolution and format**: All figures were exported at **≥1200 dpi resolution** in both raster (PNG) and vector (SVG) formats. PNG files were generated using `png(filename, width=W, height=H, res=1200, family="Arial")`, and SVG files were generated using `ggsave(filename, plot=p, device="svg", width=W, height=H, dpi=1200)`.

**Font specifications**: All text labels, axis labels, and legends used **Arial font family** (sans-serif) with appropriate sizing: titles (size=14, face="bold"), axis labels (size=12, face="bold"), axis text (size=10), legend text (size=9). Font loading was implemented using `sysfonts::font_add(family="Arial", regular="C:/Windows/Fonts/arial.ttf")` and `showtext::showtext_auto(enable=TRUE)`.

**Color schemes**: All figures used Nature-style color schemes: (i) sequential data (e.g., habitat suitability): viridis color scale (`viridis::viridis(n=256, option="D")`), (ii) categorical data (e.g., model types): Nature color palette (Maxent="#E41A1C", RF="#4DAF4A", GAM="#984EA3", NN="#377EB8"), (iii) diverging data (e.g., change maps): red-white-blue color scale.

**Language**: All figure labels, axis labels, legends, and captions were in **English** to meet international publication standards. No Chinese characters were included in any figures.

**Spatial projections**: All spatial maps were projected to Albers Conic Equal Area (centered on 105°E, 35°N) and clipped to China's national boundary for consistent geographic representation.

## 2.8 Software and Computational Environment

All analyses were conducted in **R version ≥4.0.0** using reproducible scripted workflows. Key R packages and versions included:

- **Data manipulation**: `tidyverse` (version 1.3.2), `data.table` (version 1.14.8)
- **Spatial analysis**: `sf` (version 1.0-9), `terra` (version 1.7-18), `raster` (version 3.6-20), `ncdf4` (version 1.21)
- **Species distribution modeling**: `maxnet` (version 0.1.4), `randomForest` (version 4.7-1.1), `mgcv` (version 1.8-42), `nnet` (version 7.3-19), `dismo` (version 1.3-9)
- **Causal inference**: `bnlearn` (version 4.8.3), `pcalg` (version 2.7-8), `DoubleML` (version 0.6.2), `grf` (version 2.3.0)
- **Model evaluation**: `pROC` (version 1.18.0), `caret` (version 6.0-93)
- **Visualization**: `ggplot2` (version 3.4.2), `viridis` (version 0.6.2), `ggraph` (version 2.1.0), `patchwork` (version 1.1.2)
- **Statistical analysis**: `corrplot` (version 0.92), `usdm` (version 1.1-18)

**Computational resources**: Analyses were performed on a Windows 10 system with 32GB RAM and multi-core CPU. Model training utilized parallel processing where applicable (e.g., Random Forest with `parallel=TRUE`). Total computation time: ~5 hours for data preparation, ~2 hours for model training, ~3 hours for causal discovery (300 bootstrap replicates), ~1 hour for future projections, and ~2 hours for visualization (total: ~13 hours).

**Reproducibility**: All random seeds were fixed (`set.seed(20251024)`) across all stochastic procedures (data partitioning, bootstrap resampling, model training) to ensure reproducibility. All scripts, data, and outputs are available in the project repository (`scripts/`, `output/`, `figures/`).

## 2.9 Statistical Assumptions and Limitations

### 2.9.1 Causal Discovery Assumptions

Causal discovery algorithms (PC, Hill-Climbing) make several key assumptions:

**(i) Causal sufficiency**. The algorithms assume that all common causes (confounders) of observed variables are included in the dataset. Unmeasured confounders can lead to spurious edges or incorrect edge orientations. We mitigated this risk by including comprehensive environmental variables (47 variables spanning four domains) and by using bootstrap stability assessment to filter spurious edges.

**(ii) Acyclicity**. The algorithms assume that causal relationships form a directed acyclic graph (DAG), excluding feedback loops and cycles. While this assumption is reasonable for environmental variables over short timescales, long-term feedbacks (e.g., species distribution affecting land cover through ecosystem processes) may violate this assumption.

**(iii) Distributional assumptions**. The PC algorithm assumes approximate Gaussianity for reliable partial correlation tests. We addressed this by standardizing all variables (z-score transformation) and by using score-based methods (Hill-Climbing with BIC-G) that are more robust to distributional violations.

**(iv) Sample size requirements**. Causal discovery requires sufficient sample size for reliable conditional independence tests. With n=1603 training samples and 47 variables, our sample-to-variable ratio (34:1) meets recommended thresholds (>10:1) for stable causal discovery.

### 2.9.2 Species Distribution Modeling Assumptions

SDM algorithms make several assumptions:

**(i) Equilibrium assumption**. Models assume that species distributions are in equilibrium with current environmental conditions, ignoring dispersal limitations and historical contingencies. This assumption may be violated for recently introduced species or species with limited dispersal capacity.

**(ii) Stationarity assumption**. Models assume that species-environment relationships remain constant across space and time. This assumption may be violated if ecological niches shift due to biotic interactions, phenotypic plasticity, or evolutionary adaptation.

**(iii) Presence-background assumption**. Maxent and other presence-background models assume that background points represent "available" habitat rather than "suitable but unoccupied" habitat. We addressed this by restricting background points to river network pixels, ensuring they represent available aquatic habitat.

### 2.9.3 Future Projection Limitations

Future projections are subject to several limitations:

**(i) Extrapolation risk**. Despite scenario-consistent retraining, projections may still extrapolate beyond observed environmental conditions if future climates exceed historical ranges. We mitigated this risk by using conservative emission scenarios (SSP126, SSP245) and by reporting uncertainty quantiles.

**(ii) Static biotic interactions**. Models assume that biotic interactions (competition, predation) remain constant, ignoring potential changes in species assemblages under climate change.

**(iii) Single GCM limitation**. We used a single GCM (BCC-CSM2-MR) due to data availability. Incorporating multiple GCMs would enable quantification of climate model uncertainty, refining the dominance hierarchy we observed (structural > scenario uncertainty).

**(iv) Static land use**. Future projections assume static land use, ignoring potential land-use change under SSP scenarios. Dynamic land-use scenarios (e.g., SSP-consistent urban expansion) could capture anthropogenic feedbacks absent in static projections.

## 2.10 Data Availability and Code Reproducibility

All occurrence data, environmental rasters, modeling outputs, and analysis scripts supporting the findings are available in the project repository. Key data files include:

- **Model evaluation**: `output/08_model_evaluation/evaluation_summary.csv`
- **Current predictions**: `output/11_prediction_maps/prediction_summary.csv` and `rasters/pred_*[model]_river.tif`
- **Variable importance & SHAP**: `output/09_variable_importance/importance_summary.csv` and `shap/shap_global_*.csv`
- **Response curves & ALE**: `output/10_response_curves/ale/ale_summary.csv`
- **Causal discovery**: `output/14_causal/edges_summary.csv`, `ate_summary.csv`, `cate_summary.csv`
- **Future projections**: `output/15_future_env/prediction_trends_all_models.csv`
- **Uncertainty maps**: `output/12_uncertainty/uncertainty_summary.csv` and spatial rasters
- **Causal retraining**: `output/15b_causal_retraining/performance_comparison.csv`

All analyses were conducted using reproducible R scripts organized in the `scripts/` directory. Core scripts include:

- Data preparation & collinearity: `scripts/01_data_preparation_NEW.R`, `scripts/04_collinearity_analysis.R`
- Model training: `scripts/05_model_maxnet.R`, `scripts/06_model_rf.R`, `scripts/07_model_gam.R`, `scripts/05b_model_nn.R`
- Model evaluation: `scripts/08_model_evaluation.R`
- Variable importance & SHAP: `scripts/09_variable_importance_viz.R`, `scripts/11c_shap_contrib_maps.R`
- Response curves: `scripts/10_response_curves.R`
- Current predictions: `scripts/11_current_prediction_maps.R`
- Causal discovery: `scripts/14_causal_discovery.R`, `scripts/14c_batch_ate_estimation.R`
- Causal retraining: `scripts/15b_causal_informed_retraining.R`
- Future projections: `scripts/15_future_env_projection.R`
- Uncertainty quantification: `scripts/12_uncertainty_map.R`

All scripts include detailed comments, logging output, and error handling to ensure reproducibility and transparency.

